---
layout: blog-post
comments: true
title: Thoughts on Deep Learning
category: life
---

What we are doing is to model the world, with prior. Two instances illustrating this view-point are in below.

The first instance is the model for discovering the quantum mechanics, introduced in [1901.11103](https://arxiv.org/pdf/1901.11103.pdf). Therein, they modeled the (maybe simulated) data of quantum experiments by assuming, thus a prior, that the encoding obeys a differential equation depending only on the observable, i.e., potential. (The encoding is then found as the wave-function.) The non-determined parts are all modeled by neural networks, as functions of universality.

The second instance is the [transformer](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html) for natural language process, modeling the attention phenonmenon in reality. And further, its extention, [universal transformer](https://ai.googleblog.com/2018/08/moving-beyond-translation-with.html) models the repeated attention on ambiguous part of sentence. As the same, the non-determined parts are all modeled by nerual networks, as functions of universality.

The transformer, as well as its extension, thus is a simulation of human recognization of natural language process, as the prior, while leaving the non-determined parts determined by machine, searching the most optimized possibility.
